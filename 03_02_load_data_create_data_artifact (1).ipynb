{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ewI3se7iuXt3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2159a048-8e30-4ae4-ca40-0ea53a537eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymilvus==2.3.4\n",
            "  Downloading pymilvus-2.3.4-py3-none-any.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain==0.0.352\n",
            "  Downloading langchain-0.0.352-py3-none-any.whl (794 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.4/794.4 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==1.6.1\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytube==15.0.0\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting youtube-transcript-api==0.6.1\n",
            "  Downloading youtube_transcript_api-0.6.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyarrow==14.0.2 in /usr/local/lib/python3.10/dist-packages (14.0.2)\n",
            "Collecting typing_extensions==4.9.0\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting comet-ml==3.35.5\n",
            "  Downloading comet_ml-3.35.5-py3-none-any.whl (586 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.8/586.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<=1.58.0,>=1.49.1 (from pymilvus==2.3.4)\n",
            "  Downloading grpcio-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus==2.3.4) (3.20.3)\n",
            "Collecting environs<=9.5.0 (from pymilvus==2.3.4)\n",
            "  Downloading environs-9.5.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting ujson>=2.0.0 (from pymilvus==2.3.4)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus==2.3.4) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pymilvus==2.3.4) (2.31.0)\n",
            "Collecting minio>=7.0.0 (from pymilvus==2.3.4)\n",
            "  Downloading minio-7.2.5-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.352) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.352) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.352) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.352) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.352)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.352)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain==0.0.352)\n",
            "  Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain==0.0.352)\n",
            "  Downloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.70 (from langchain==0.0.352)\n",
            "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.352) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.352) (2.6.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.352) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.6.1) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.6.1)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.6.1) (4.66.2)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (4.19.2)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (5.9.5)\n",
            "Requirement already satisfied: python-box<7.0.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (6.1.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (1.0.0)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (1.41.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (3.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (2.0.7)\n",
            "Requirement already satisfied: websocket-client<1.4.0,>=0.55.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (1.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (1.14.1)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (3.0.3)\n",
            "Requirement already satisfied: everett[ini]<3.2.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (3.1.0)\n",
            "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (0.21.7)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml==3.35.5) (13.7.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.352) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.352) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.352) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.352) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.352) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.6.1) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.6.1) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.352)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.352)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting python-dotenv (from environs<=9.5.0->pymilvus==2.3.4)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml==3.35.5) (5.0.8)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.6.1) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.6.1)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.6.1)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.352)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml==3.35.5) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml==3.35.5) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml==3.35.5) (0.18.0)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain==0.0.352)\n",
            "  Downloading langchain_community-0.0.26-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.23-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.22-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.21-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-core<0.2,>=0.1 (from langchain==0.0.352)\n",
            "  Downloading langchain_core-0.1.29-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_core-0.1.28-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_core-0.1.27-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_core-0.1.25-py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_core-0.1.24-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.3/241.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.70 (from langchain==0.0.352)\n",
            "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain==0.0.352) (23.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from minio>=7.0.0->pymilvus==2.3.4) (23.1.0)\n",
            "Collecting pycryptodome (from minio>=7.0.0->pymilvus==2.3.4)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus==2.3.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus==2.3.4) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.352) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.352) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pymilvus==2.3.4) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml==3.35.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml==3.35.5) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.352) (3.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml==3.35.5) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.352)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->minio>=7.0.0->pymilvus==2.3.4) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio>=7.0.0->pymilvus==2.3.4) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio>=7.0.0->pymilvus==2.3.4) (2.21)\n",
            "Installing collected packages: ujson, typing_extensions, pytube, python-dotenv, pycryptodome, mypy-extensions, marshmallow, jsonpointer, h11, grpcio, youtube-transcript-api, typing-inspect, jsonpatch, httpcore, environs, httpx, dataclasses-json, openai, minio, langsmith, comet-ml, pymilvus, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.62.0\n",
            "    Uninstalling grpcio-1.62.0:\n",
            "      Successfully uninstalled grpcio-1.62.0\n",
            "  Attempting uninstall: comet-ml\n",
            "    Found existing installation: comet-ml 3.39.0\n",
            "    Uninstalling comet-ml-3.39.0:\n",
            "      Successfully uninstalled comet-ml-3.39.0\n",
            "Successfully installed comet-ml-3.35.5 dataclasses-json-0.6.4 environs-9.5.0 grpcio-1.58.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.352 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 marshmallow-3.21.1 minio-7.2.5 mypy-extensions-1.0.0 openai-1.6.1 pycryptodome-3.20.0 pymilvus-2.3.4 python-dotenv-1.0.1 pytube-15.0.0 typing-inspect-0.9.0 typing_extensions-4.9.0 ujson-5.9.0 youtube-transcript-api-0.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "comet_ml"
                ]
              },
              "id": "a124a689d5ca41a2adde2c1f4cd072fc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \\\n",
        "  pymilvus==2.3.4 \\\n",
        "  langchain==0.0.352 \\\n",
        "  openai==1.6.1 \\\n",
        "  pytube==15.0.0 \\\n",
        "  youtube-transcript-api==0.6.1 \\\n",
        "  pyarrow==14.0.2 \\\n",
        "  typing_extensions==4.9.0 \\\n",
        "  comet-ml==3.35.5\n",
        "\n",
        "# Restart the runtime after pip installing (CTRL + M)  Otherwise, the runtime\n",
        "# remembers the old version of pyArrow and causes issues for pyMilvus\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import (MilvusClient\n",
        "                      , connections\n",
        "                      , Collection\n",
        "                      , CollectionSchema\n",
        "                      , FieldSchema\n",
        "                      , DataType\n",
        "                      , utility)\n",
        "import json\n",
        "\n",
        "\n",
        "COLLECTION_NAME = 'youtube'\n",
        "EMBEDDING_DIMENSION = 1536  # Embedding vector size in this example\n",
        "ZILLIZ_CLUSTER_URI = 'YOUR ZILLIZ URI'  # Endpoint URI obtained from Zilliz Cloud\n",
        "ZILLIZ_API_KEY = 'YOUR ZILLIZ API KEY'\n",
        "\n",
        "YT_VIDEO_URLS = [\n",
        "    \"https://www.youtube.com/watch?v=Q4OBx3S0Ysw&t=118s\",\n",
        "    \"https://youtu.be/4OZip0cgOho?si=KHUsA4J8L3rbZAAZ\"]\n",
        "\n",
        "# Connect to the zilliz cluster\n",
        "connections.connect(uri=ZILLIZ_CLUSTER_URI, token=ZILLIZ_API_KEY, secure=True)\n",
        "\n",
        "client = MilvusClient(\n",
        "    uri=ZILLIZ_CLUSTER_URI,\n",
        "    token=ZILLIZ_API_KEY)\n",
        "\n",
        "# Remove any previous collections with the same name\n",
        "if utility.has_collection(COLLECTION_NAME):\n",
        "    utility.drop_collection(COLLECTION_NAME)\n",
        "\n",
        "# Create collection which includes the id, title, and embedding.\n",
        "fields = [\n",
        "  FieldSchema(name='id', dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=36),\n",
        "  FieldSchema(name='video_id', dtype=DataType.INT64,),\n",
        "  FieldSchema(name='title', dtype=DataType.VARCHAR, description='Title texts', max_length=500),\n",
        "  FieldSchema(name='author', dtype=DataType.VARCHAR, description='Author', max_length=200),\n",
        "  FieldSchema(name='part_id', dtype=DataType.INT64),\n",
        "  FieldSchema(name='max_part_id', dtype=DataType.INT64),\n",
        "  FieldSchema(name='text', dtype=DataType.VARCHAR, description='Text of chunk', max_length=2000),\n",
        "  FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, description='Embedding vectors', dim=EMBEDDING_DIMENSION)\n",
        "]\n",
        "\n",
        "schema = CollectionSchema(fields=fields)\n",
        "\n",
        "collection = Collection(name=COLLECTION_NAME, schema=schema)\n",
        "\n",
        "# Create an index for the collection.\n",
        "index_params = {\n",
        "    'index_type': 'AUTOINDEX',\n",
        "    'metric_type': 'IP',\n",
        "    'params': {}\n",
        "}\n",
        "\n",
        "\n",
        "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
        "\n"
      ],
      "metadata": {
        "id": "SxtZXNoewwpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf0951a-fa31-4394-fbb2-e04e3b02666c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:pymilvus.milvus_client.milvus_client:Created new connection using: 62c844c69a1f4c29a8146d1ea1f1d3d8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Status(code=0, message=)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from pymilvus import MilvusClient, connections\n",
        "from uuid import uuid4\n",
        "from langchain.document_loaders import YoutubeLoader\n",
        "import youtube_transcript_api\n",
        "import pytube\n",
        "\n",
        "connections.connect(uri=ZILLIZ_CLUSTER_URI, token=ZILLIZ_API_KEY, secure=True)\n",
        "\n",
        "client = MilvusClient(\n",
        "    uri=ZILLIZ_CLUSTER_URI,\n",
        "    token=ZILLIZ_API_KEY)\n",
        "\n",
        "\n",
        "\n",
        "openai_client = OpenAI(\n",
        "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        "    api_key=\"YOUR OPENAI API KEY\",\n",
        ")\n",
        "\n",
        "# Extract embedding from text using OpenAI  string -> vector\n",
        "# This function is directly from https://docs.zilliz.com/docs/similarity-search-with-zilliz-cloud-and-openai, but with \"text-embedding-ada-002\" added.\n",
        "def create_embedding_from_string(text):\n",
        "    return openai_client.embeddings.create(\n",
        "        input=text,\n",
        "        model='text-embedding-ada-002').data[0].embedding\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size = 1000,\n",
        "  chunk_overlap  = 50,\n",
        "  length_function = len,\n",
        "  add_start_index = True,\n",
        ")\n",
        "\n",
        "for video_id, url in enumerate(YT_VIDEO_URLS):\n",
        "\n",
        "  yt_data = YoutubeLoader.from_youtube_url(url, add_video_info=True).load()[0]\n",
        "  video_parts = text_splitter.create_documents([yt_data.page_content])\n",
        "\n",
        "  for part_id, part in enumerate(video_parts):\n",
        "      id = str(uuid4())\n",
        "      print(f'uplading document {id}... {yt_data.metadata[\"title\"]}')\n",
        "      client.insert(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        data={\n",
        "            'id': id,\n",
        "            'video_id': video_id,\n",
        "            'title': yt_data.metadata['title'],\n",
        "            'author': yt_data.metadata['author'],\n",
        "            'part_id': part_id,\n",
        "            'max_part_id': len(video_parts),\n",
        "            'text': part.page_content,\n",
        "            'embedding': create_embedding_from_string(part.page_content)\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVOv0eDFXjLM",
        "outputId": "7d9e3c7b-fa98-4a94-8a9f-142b7b64d469"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:pymilvus.milvus_client.milvus_client:Created new connection using: 9fbc8ca209b149cc8bc0043da05f882b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uplading document f81e3520-153d-402f-87b6-ff0c4e66994b... Vector Similarity Search using Images with Zilliz\n",
            "uplading document e89162f4-8ed8-49a6-ab51-781caff8e87a... Vector Similarity Search using Images with Zilliz\n",
            "uplading document 0f146ba0-e625-4850-9695-bf31c7743652... Vector Similarity Search using Images with Zilliz\n",
            "uplading document bf00d2d7-8ddf-4905-b0e0-4d6b083d24c4... Vector Similarity Search using Images with Zilliz\n",
            "uplading document 911ff9f6-8be2-44b8-9c49-f650c2c90a93... Vector Similarity Search using Images with Zilliz\n",
            "uplading document 269a2e29-c6b3-473b-91ea-dd58e6460e83... Vector Similarity Search using Images with Zilliz\n",
            "uplading document 74a2fd45-15f7-4186-9f77-87310b907d71... Vector Similarity Search using Images with Zilliz\n",
            "uplading document 5a6c877e-39c6-4464-a1cf-feb2d333ea9f... Vector Similarity Search using Images with Zilliz\n",
            "uplading document e8eb56d8-4947-4e49-a4d1-c19a1b76eb41... Vector Similarity Search using Images with Zilliz\n",
            "uplading document 33b74bd9-3a95-4d8c-8bf7-618a9b419946... How I Would Learn Data Science (If I Had to Start Over)\n",
            "uplading document cc2b8e19-c9de-44cf-902e-a3bf411c7af8... How I Would Learn Data Science (If I Had to Start Over)\n",
            "uplading document cb822960-85fd-4407-83bd-456b6ae80221... How I Would Learn Data Science (If I Had to Start Over)\n",
            "uplading document 5efeace9-e6c6-47ca-b157-90a41b4067c6... How I Would Learn Data Science (If I Had to Start Over)\n",
            "uplading document af936c68-d491-4b01-b42a-30f4b16f1dff... How I Would Learn Data Science (If I Had to Start Over)\n",
            "uplading document 79196f6e-92dc-4bd2-9c42-8a14d7c59d55... How I Would Learn Data Science (If I Had to Start Over)\n",
            "uplading document 949dd6ad-0588-4ea6-9586-5c16ea81f27f... How I Would Learn Data Science (If I Had to Start Over)\n",
            "uplading document 5012e6d9-72ab-438f-aeb5-9cd59a66c333... How I Would Learn Data Science (If I Had to Start Over)\n",
            "uplading document 841b6673-c8b3-4468-819d-0d291522a9ef... How I Would Learn Data Science (If I Had to Start Over)\n",
            "uplading document 46bdd965-38bc-495d-b93d-893d0d3de9dc... How I Would Learn Data Science (If I Had to Start Over)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The collection must be \"LOADED\" in zilliz for this to work.\n",
        "results = collection.query(\n",
        "    expr='title != \"none\"',\n",
        "    output_fields=['title', 'author', 'part_id', 'max_part_id', 'text'])\n",
        "\n",
        "with open('data.json', 'w') as file:\n",
        "  file.write(json.dumps(results, indent=2))"
      ],
      "metadata": {
        "id": "aeBj9xabXfyN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## First, we’ll pip install the Comet library\n",
        "!pip install comet_ml\n",
        "\n",
        "## import comet_ml\n",
        "from comet_ml import Experiment\n",
        "\n",
        "## Create an experiment with your api key\n",
        "experiment = Experiment(\n",
        "    api_key='YOUR COMET API KEY',\n",
        "    project_name='youtube_transcriptions',\n",
        "    workspace='YOUR USERNAME'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkQE4p2cpsqY",
        "outputId": "8b8b14e7-4c61-4ac5-9aee-9dfdc3d327ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.10/dist-packages (3.35.5)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (4.19.2)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (5.9.5)\n",
            "Requirement already satisfied: python-box<7.0.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (6.1.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.31.0)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.41.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from comet_ml) (3.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.0.7)\n",
            "Requirement already satisfied: websocket-client<1.4.0,>=0.55.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.14.1)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (3.0.3)\n",
            "Requirement already satisfied: everett[ini]<3.2.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (3.1.0)\n",
            "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (0.21.7)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (13.7.1)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.10/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (5.0.8)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/machine-learning-upgrade/youtube-transcriptions/be526e7183d94f949fc4421504c66a74\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "artifact = Artifact(name=\"milvus-query-results\", artifact_type=\"dataset\")\n",
        "artifact.add(\"data.json\")\n",
        "\n",
        "experiment.log_artifact(artifact)\n",
        "experiment.end()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq34gBPbucza",
        "outputId": "5a99f121-d980-46a0-e632-1d8d17993158"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/machine-learning-upgrade/youtube-transcriptions/22a5d0e5eae144b6892606cd88b49e69\n",
            "\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Artifact 'milvus-query-results' version 10.0.0 created (previous was: 9.0.0)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Scheduling the upload of 1 assets for a size of 21.01 KB, this can take some time\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Artifact 'machine-learning-upgrade/milvus-query-results:10.0.0' has started uploading asynchronously\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/machine-learning-upgrade/youtube-transcriptions/22a5d0e5eae144b6892606cd88b49e69\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     artifact assets     : 1 (21.01 KB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     artifacts           : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Artifact 'machine-learning-upgrade/milvus-query-results:10.0.0' has been fully uploaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from langchain.document_loaders import YoutubeLoader\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=Q4OBx3S0Ysw&t=118s\", add_video_info=True\n",
        ")\n",
        "\n",
        "data = loader.load()\n",
        "data[0].page_content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "6QK8fahg58gp",
        "outputId": "00374e6b-b318-4c19-b414-1b0ff2e8572f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"foreign [Applause] to perform Vector similarity search on images zillas is a vector database it is designed to handle massive data sets containing vectors vectors are just numerical representations of data so this is your text documents audio and that also includes images traditional methods of searching through these large data sets has been consuming and computationally expensive so zilis uses Advanced algorithms and data structures tailored specifically for Vector similarity search instead of comparing each point individually zilla's organizes these vectors in a way that optimizes similarity queries this allows you to quickly find items that are similar to a given query vector so is this focused on cutting-edge Technologies for data indexing storage and retrieval with an emphasis on GPU accelerated computing for a high level overview of this project we are going to get the connection from The Notebook to Melvis and set up a cluster we're going to import images from Google Drive I'll make sure to leave a link in the description we're going to set up zilla's Cloud we're going to insert our data and then we will do our similarity search using a resnet 50 model and then we're going to go back again and do it with resnet 152 to just compare the output and see how it's different to get started the first thing that you'll want to do is set yourself up azilla's account when you do that you will have a hundred dollars worth of free credits so that you'll be able to follow along with this demo for free I'm going to create a cluster and then we'll go over downloading the images creating the embeddings modeling those but then we'll be able to write those embeddings over to the milvis database I will be using the starter serverless plan and I'm just going to give this the name image search so that's the name of my collection the metric type there are two options and I'm going to be choosing this L2 which is just your standard euclidean distance all right so now I am going to put the public endpoint in my code I already have the API key over there copy this and you'll see that image search has been set up but that it doesn't actually have any data for us I'm going to add my URI [Music] like I said my API key is already here all right so now we're going to dive into this demo here is a Google Drive link that you can use to access the images that I'm using they are images of my family but if you want to go along with your own images that'll work too so we're starting like we normally do by pip installing our packages we'll restart our run time and then now I am just going to be setting up my directory to match the docs and so we are in the content folder and then I'll add one for python as well oh okay so now this should work all right so we are going to pip install a couple more libraries Pine milfus is going to be used to connect to zilla's Cloud we're going to use torch to run the embedding model we will use torch vision for the actual model and pre-processing and then G down is for working with Google Drive and the tqdm package is so that we get those cute little loading bars while our model is training then we will import these libraries here are the docs for starting here these docs are what I followed to put this together and so here you're going to need to add the Google Drive Link so you could use the link above open that up actually let me make sure that that works then we are going to set the output path and file name for the downloaded file and then download the file from the given Google Drive Link using gdam all right perfect so that link I gave you works next we're just going to be setting our parameters so we had given the collection name image search you had already seen me put my URI and I had already put my API key here and then now we need to set up our cloud so we will be first connecting to the zillas cloud cluster using the URI that we had there it's just if the collection already exists drop it okay and now we're going to be setting up our schema so it's going to have the ID the file path and the image embedding and we will now create an index on that collection here is that L2 euclidean distance metric that we already talked about and this is just to get the file path of the different images we have 1619 images okay so here we are going to create our base model using resnet 50. and this is actually not creating the model it is defining the model if you haven't used torch before I learned this the first time that I use torch is that they actually build their neural net models sequentially by adding the different layers so that's where we get this sequential function here we're creating a preprocessor for making sure the images are the same size so we're going to resize crop normalize or at least we're setting up a preprocessor so we didn't actually do those steps here but we created a function that will allow us to do that now we're going to insert the data so this one takes a minute to run but we're going to embed the function that embeds the batch and inserts it then we're going to read the images into batches for embedding and insertion and here we're actually going to insert the data all right I hope you're ready because we are about to perform our search now for this I have already placed four images that all looked different that we can do our search on the code here is going to go through and embed the images so this is the process of converting these photos into vectors using our resnet 50 model and we are going to iterate through and use that preprocessor that we set up that'll crop and normalize the images um then we're going to do the search through these embeddings for similar images and then at the end um it'll be using matplotlib to set up a visual display for us that will list the search time and the distance of the chosen searched photo to the image that I provided okay so let's get started all right perfect so we have our output and I am just going to store this so that we are able to do a comparison in a little bit once we have the output from our next model okay and now I'm just going to go through and actually update this to be using resnet 152 and rerun [Music] awesome now we get to go take a look at these results I actually went back and ran this a couple more times myself and found that in general the resnet 152 model had shorter search times or shorter distances but that was not always the case at all and even in our results we're going to see a lot of instances where the resnet 50 has shorter distances here though we will see that the resnet 152 model the search time was about three times faster than resnet 152. taking a look at the first photo we see that all the distances for the resnet 152 model are shorter compared to the resnet 50 model and really the only photo that looks different here is the last result in the second row of results the distances are actually larger with the resnet 152 model and you can really see this because the second result returned from the 152 results my daughter has her arms in and we'd assume that the ones that have the arms out are going to be more similar to the given photo in the third row of results it's my daughter and husband with their arms up wearing coats and the distances again they're higher with the resnet 152 model even though these are super close results and you get to benefit from the faster search time in the fourth row of results all the images look really similar with the exception of where is my son placed because you can't really see him in the given photo and then in the search results he's sort of in different places the distance for the first result is very close but then the distance for the resnet 152 model is much larger for the other two photos but again the search was quicker I hope you enjoyed this demo where we did Vector similarity search using images be sure to like And subscribe and I look forward to seeing you in my next demo\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}