{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dictionary containing prompts for different types of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"math\": \"\"\"Please answer the following mathematics question. If you don't know the answer, respond \"I don't know.\" \\n Question: {question}\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define system prompts associated with different prompt types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPTS = {\n",
    "    \"math\": \"You are a helpful assistant who solves math problems for users.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate message for the AI chat system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_messages(prompt_id, system_prompt_id =None, prompt_variables = {}):\n",
    "    user_prompt = PROMPTS[prompt_id].format(**prompt_variables)\n",
    "    system_prompt = SYSTEM_PROMPTS[prompt_id] if system_prompt_id is None else SYSTEM_PROMPTS[system_prompt_id]\n",
    "    #Return system and user messages in a list format\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function to interact with the GPT-4 API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import comet_llm\n",
    "openai_client = OpenAI(api_key=\"YOUR-API-KEY\")\n",
    "comet_llm.init(api_key=\"YOUR-COMET-API-KEY\")\n",
    "def get_completion(\n",
    "    prompt_id,\n",
    "    system_prompt_id = None,\n",
    "    prompt_variables = None,\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "):\n",
    "    #Generate messages using the provided inputs\n",
    "    messages = generate_messages(prompt_id, system_prompt_id, prompt_variables)\n",
    "\n",
    "    #Call an OpenAI function to get completions based on the generated messages.\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    #Log the prompt, completion, and related metadata\n",
    "    comet_llm.log_prompt(\n",
    "        prompt=messages[1]['content'],\n",
    "        prompt_template=PROMPTS[prompt_id],\n",
    "        prompt_template_variables=prompt_variables,\n",
    "        metadata={\n",
    "            \"usage.prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"usage.completion_tokens\": response.usage.completion_tokens,\n",
    "            \"usage.total_tokens\": response.usage.total_tokens,\n",
    "            \"system_fingerprint\" response.system_fingerprint\n",
    "        },\n",
    "        output=response.choices[0].message.content,\n",
    "    )\n",
    "    #return the response\n",
    "    return response.choices[0].message.conten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = {\n",
    " \"question\": \"What three-digit palindromes are also perfect squares?\"\n",
    "}\n",
    "get_completion(prompt_id=\"math\", prompt_variables=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "client = OpenAI(api_key=\"YOUR-API-KEY\")\n",
    "\n",
    "def get_completion(\n",
    "    prompt,\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    return_full=False,\n",
    "    **kwargs\n",
    "):\n",
    "    response = client.completions.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    if return_full:\n",
    "        return response\n",
    "\n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "class Pipeline(ABC):\n",
    "    @abstractmethod\n",
    "    def run(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptNode(Node):\n",
    "    def __init__(self, prompt_template, generate=get_completion):\n",
    "        self.prompt_template = prompt_template\n",
    "        self.generate = generate\n",
    "        self.prompt = None\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        self.prompt = self.prompt_template.format(**kwargs)\n",
    "        return self.generate(self.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_tigrinya = \"\"\"Translate the following into Tigrinya:\n",
    "{prompt} => \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TigrinyaTranslatePipeline(Pipeline):\n",
    "    def __init__(self):\n",
    "        self.p1 = PromptNode(prompt_template=translate_tigrinya)\n",
    "    def run(self, **kwargs):\n",
    "        return self.p1.forward(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_icl = TigrinyaTranslatePipeline()\n",
    "no_icl.run(prompt=\"It was the best of times, it was the worst of times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_tigrinya_icl = \"\"\"Translate the following into Tigrinya:\n",
    "It was the age of wisdom, it was the age of foolishness. => ዘመነ ጥበብ እዩ ነይሩ፣ ዘመን ዕሽነት እዩ ነይሩ።\n",
    "\n",
    "Translate the following into Tigrinya:\n",
    "To be, or not to be, that is the question. => ምዃንን ዘይምህላውን ንሱ እዩ እቲ ሕቶ።\n",
    "\n",
    "Translate the following into Tigrinya:\n",
    "What happiness was ours that day, what joy, what rest, what hope, what gratitude, what bliss! => ኣብታ መዓልቲ እቲኣ ከመይ ዝበለ ሓጐስ እዩ ነይሩ፣ ከመይ ዝበለ ሓጐስ፣ ከመይ ዝበለ ዕረፍቲ፣ ከመይ ዝበለ ተስፋ፣ ከመይ ዝበለ ምስጋና፣ ከመይ ዝበለ ዕግበት!\n",
    "\n",
    "Translate the following into Tigrinya:\n",
    "{prompt} => \"\"\"\n",
    "\n",
    "class TigrinyaTranslatePipeline(Pipeline):\n",
    "    def __init__(self, icl=None):\n",
    "        if icl == 'icl' :\n",
    "            self.p1 = PromptNode(prompt_template=translate_tigrinya_icl)\n",
    "        else:\n",
    "            self.p1 = PromptNode(prompt_template=translate_tigrinya)\n",
    "    \n",
    "    def run(self, **kwargs):\n",
    "        return self.p1.forward(**kwargs)\n",
    "\n",
    "icl = TigrinyaTranslatePipeline(icl=\"icl\")\n",
    "icl.run(prompt=\"It was the best of times, it was the worst of times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediary Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_template = \"\"\"INSTRUCTION:\n",
    "Solve the following equation: {prompt}\n",
    "\n",
    "RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "class EquationPipeline(Pipeline):\n",
    "    def __init__(self, with_zero_shot_cot=False):\n",
    "    if with_zero_shot_cot is True:\n",
    "        self.p1 = PromptNode(prompt_template=math_template + \"Let's think step by step. \")\n",
    "    else:\n",
    "        self.p1 = PromptNode(prompt_template=math_template)\n",
    " \n",
    "    def run(self, **kwargs):\n",
    "        return self.p1.forward(**kwargs)\n",
    "        \n",
    "equation = \"6^8 * 2 / 3 + 7 - 1 =\"\n",
    "raw_pipeline = EquationPipeline()\n",
    "cot_pipeline =\n",
    "EquationPipeline(with_zero_shot_cot=True)\n",
    "print(\"Without Chain of Thought Prompting\")\n",
    "print(raw_pipeline.run(prompt=equation))\n",
    "print(\"With Chain of Thought Prompting\")\n",
    "print(cot_pipeline.run(prompt=equation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 YouTube Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_search import YoutubeSearch\n",
    "\n",
    "class YouTubeRetriever(Node):\n",
    "    def __init__(self, generate=get_completion):\n",
    "        self.generate = generate\n",
    "\n",
    "    def _fetch_transcripts(self, query):\n",
    "        results = YoutubeSearch(query, max_results=10).to_dict()\n",
    "        return [ f\"{['url_suffix'].split('&')[0]}\" for x in results]\n",
    "\n",
    "    def _parse_transcript(self, transcript, video_id):\n",
    "        full_text = \"\"\n",
    "        arr = transcript[0][video_id]\n",
    "        for obj in arr:\n",
    "            full_text += f\"{obj['text']} \"\n",
    "        \n",
    "        return full_text\n",
    "\n",
    "    def _summarize_transcript(self, transcript):\n",
    "        summary = self.generate(prompt=f\"\"\"INSTRUCTION: \\nBelow is a transcript generated from a YouTube video. Condense and summarize it.\\n\\n\"{transcript}\"\\nRESPONSE:\\n\"\"\").strip()\n",
    "        return summary\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = \"\"\n",
    "        # Generate search term + strip leading/trailing newlines and quotation marks\n",
    "        youtube_query = self.generate(prompt=youtube_query_template.format(prompt=question)).strip().strip(\"\\\"\")\n",
    "        results = YoutubeSearch(youtube_query, max_results=10).to_dict()\n",
    "        for x in results:\n",
    "            video_id = x['id']\n",
    "            transcript = \"\"\n",
    "            try:\n",
    "                transcript = YouTubeTranscriptApi.get_transcripts(video_ids=[video_id])\n",
    "                transcript = self._parse_transcript(transcript, video_id)\n",
    "                if len(transcript) > 2000:\n",
    "                    transcript = transcript[0:2000]\n",
    "                transcript = self._summarize_transcript(transcript)\n",
    "            except TranscriptsDisabled:\n",
    "                print(f\"Transcripts disabled for{x['title']}\")\n",
    "                pass\n",
    "\n",
    "            snippet = f\"{x['title']} by {x['channel']}\\n\\n{transcript}\\n\\n\"\n",
    "            context += snippet\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAWithYoutubePipeline(Pipeline):\n",
    "    def __init__(self):\n",
    "        self.context = \"\"\n",
    "        self.retriever = YouTubeRetriever()\n",
    "        self.qa = PromptNode(prompt_template=\"\"\"#INSTRUCTION:\n",
    "Below, you have summaries from several YouTube videos:\n",
    "\n",
    "{context}Use the above summaries to answer this question: {question}\n",
    "#RESPONSE:\n",
    "\"\"\")\n",
    "\n",
    "    def run(self, question):\n",
    "        self.context = self.retriever.forward(question=question)\n",
    "        return self.qa.forward(context=self.context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without pipeline (i.e. no additional context given)\n",
    "get_completion(prompt=\"What is Mixtral 8x-7b?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with pipeline\n",
    "pipe = QAWithYoutubePipeline()\n",
    "pipe.run(\"What is Mixtral 8x-7b?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate(text, lang):\n",
    "    translation = GoogleTranslator(source='auto', target=lang).translate(text, dest=lang)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslateNode(Node):\n",
    "    def __init__(self, generate_fn=get_completion):\n",
    "        self.preprocessing = \"\"\"#INSTRUCTION:\n",
    "        \n",
    "From the following text, extract the sequences that are written in {lang}:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "#RESPONSE:\n",
    "\"\"\"\n",
    "        self.generate = generate_fn\n",
    "    def forward(self, text, generate=get_completion):\n",
    "        extracted_text = self.generate(self.preprocessing.format(text=text, lang=\"en\"))[1:-1]\n",
    "        # Remove wrapping quotation marks\n",
    "        translated_text = translate(extracted_text, \"en\")\n",
    "        translated = text.replace(extracted_text, translated_text)\n",
    "        return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = {\n",
    "    \"translate\": {\n",
    "        \"description\": \"\"\"translate(text, lang) -> This function takes input text and translates it to the \"lang\" language.\"\"\",\n",
    "        \"node\": TranslateNode,\n",
    "        \"transform_q\": True\n",
    "    },\n",
    "    \"YouTubeResearch\": {\n",
    "        \"description\": \"\"\"YouTubeResearch(question) -> This function takes a question and uses YouTube to generate research around the question topic. Before using, you should translate any non-English questions into English.\"\"\",\n",
    "        \"node\": QAWithYoutubePipeline,\n",
    "        \"transform_q\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools_context = \"\"\"#INSTRUCTION: You are a helpful assistant who is capable of running Python functions. You answer questions, but you only respond in English. You have the following functions available to you as tools:\n",
    "{tools}\n",
    "Do you need a tool to answer the following question in English?\n",
    "\"{question}\"\n",
    "Respond \"yes\" or \"no\"\n",
    "#RESPONSE: \"\"\"\n",
    "\n",
    "which_tool = \"\"\"#INSTRUCTION: Which tool do you need? You can respond with {tool_names}\n",
    "#RESPONSE: \"\"\"\n",
    "\n",
    "final_q = \"\"\"#INSTRUCTION: Write a response that accurately answers the following question in English:\n",
    "\"{question}\"\n",
    "#RESPONSE: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Management for LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class QAAgent(Pipeline):\n",
    "    def __init__(self, tools=tools, generate_fn=get_completion):\n",
    "        self.generate = generate_fn\n",
    "        self.translate = TranslateNode()\n",
    "        self.youtube = QAWithYoutubePipeline()\n",
    "        self.context = \"\"\n",
    "        self.tools = tools\n",
    "        self.available_tools = copy.deepcopy(tools)\n",
    "    \n",
    "    def _intermediary_step(self):\n",
    "        formatted_tools = \"\"\n",
    "        need_tool = \"\"\n",
    "        next_tool = \"\"\n",
    "        tool_context = \"\"\n",
    "        selected_tool = None\n",
    "        self.context = \"\" # Clear context\n",
    "        for tool in self.available_tools.keys():\n",
    "            formatted_tools += self.available_tools[tool]['description']\n",
    "            formatted_tools += \"\\n\"\n",
    "            need_tool_input = tools_context.format(tools=formatted_tools, question=self.question)\n",
    "            need_tool = self.generate(need_tool_input)\n",
    "            self.context += need_tool_input + \"\\n\\n\" + need_tool + \"\\n\\n\"\n",
    "            if \"yes\" in need_tool.lower():\n",
    "                tool_names = \" or \".join(self.available_tools.keys())\n",
    "                next_tool = self.generate(self.context + which_tool.format(tool_names=tool_names))\n",
    "                self.context += which_tool.format(tool_names = tool_names) + \"\\n\\n\" + next_tool + \"\\n\\n\"\n",
    "                for name in self.available_tools.keys():\n",
    "                    if name in next_tool:\n",
    "                        selected_tool = name\n",
    "                        break\n",
    "            return selected_tool\n",
    "\n",
    "\n",
    "        def run(self, question):\n",
    "            self.question = question\n",
    "            selected_tool = self._intermediary_step()\n",
    "\n",
    "            while len(self.available_tools.keys()) > 0:\n",
    "                selected_tool = self._intermediary_step()\n",
    "                \n",
    "                if selected_tool == None:\n",
    "                    break\n",
    "\n",
    "                nxt = self.tools[selected_tool]['node']()\n",
    "\n",
    "                if hasattr(nxt, 'forward'):\n",
    "                    output = nxt.forward(self.question)\n",
    "                else:\n",
    "                    output = next.run(self.question)\n",
    "\n",
    "                if self.tools[selected_tool]['transform_q'] == True:\n",
    "                    self.question = output\n",
    "                \n",
    "                self.context += output\n",
    "                del self.available_tools[selected_tool]\n",
    "\n",
    "            self.context += final_q.format(question=self.question)\n",
    "            answer = self.generate(self.context)\n",
    "            self.available_tools = copy.deepcopy(self.tools)\n",
    "\n",
    "            return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAAgent()\n",
    "agent.run(\"¿Qué; es este proyecto LLMLingua del que todo el mundo habla?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_template = \"\"\"#INSTRUCTION:\n",
    "Write a Python function named {name} that {description}. Make sure to include all necessary imports.\n",
    "\n",
    "#RESPONSE\n",
    "\"\"\"\n",
    "\n",
    "code_gen_template_w_tests = \"\"\"#INSTRUCTION:\n",
    "Write a Python function named {name} that {description}. Make sure to include all necessary imports.\n",
    "\n",
    "The function {name} will be evaluated with the following unit tests:\n",
    "{tests}\n",
    "\n",
    "#RESPONSE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGenerateImage(unittest.TestCase):\n",
    "    def test_valid_input(self):\n",
    "        width, height = 200, 300\n",
    "        image = generate_image(f'{width}x{height}')\n",
    "        self.assertEqual(image.size, (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTS = [\n",
    "    {\n",
    "        \"name\": \"generate_image(dimensions)\",\n",
    "        \"description\": \"takes a string containing the dimensions of an image, like '200x300', and generates an image of those dimensions using 3 random colors, before finally returning the image object.\",\n",
    "        \"tests\": image_tests,\n",
    "        \"tests_class\": TestGenerateImage\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"evaluate_expression(expression)\",\n",
    "        \"description\": \"takes a string containing a mathematical equation, parses the equation, and returns its evaluated result.\",\n",
    "        \"tests\": math_tests,\n",
    "        \"tests_class\": TestEvaluateExpression\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"merge_k_lists(lists)\",\n",
    "        \"description\": \"takes an array of k linkedlists lists, each sorted in ascending order, and merges all the linked-lists into one sorted linkedlist, returning the final sorted linked-list.\",\n",
    "        \"test\": merge_k_tests,\n",
    "        \"tests_class\": TestMergeKLists\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptWithMKwargsNode(Node):\n",
    "    def __init__(self, prompt_template, generate=get_completion):\n",
    "        self.prompt_template = prompt_template\n",
    "        self.generate = generate\n",
    "        self.prompt = None\n",
    "        self.prompt_kwargs = None\n",
    "    def forward(self, model_kwargs=None, prompt_kwargs=None):\n",
    "        self.prompt_kwargs = prompt_kwargs\n",
    "        if self.prompt_kwargs != None:\n",
    "            self.prompt = self.prompt_template.format(**self.prompt_kwargs)\n",
    "        else:\n",
    "            self.prompt = self.prompt_template\n",
    "            if model_kwargs != None:\n",
    "                return self.generate(self.prompt, return_full=True, **model_kwargs)\n",
    "            else:\n",
    "                return self.generate(self.prompt,return_full=True)\n",
    "\n",
    "\n",
    "class ExecNode(Node):\n",
    "    def __init__(self):\n",
    "        self.success = True\n",
    "        self.message = None\n",
    "        \n",
    "    def forward(self, code):\n",
    "        print(code)\n",
    "        compiled = compile(code, 'test', 'exec')\n",
    "        try:\n",
    "            exec(compiled)\n",
    "        except Exception as e:\n",
    "            self.success = False\n",
    "            self.message = e\n",
    "            pass\n",
    "        return self.success\n",
    "\n",
    "class EvaluateNode(Node):\n",
    "    def __init__(self, test_case):\n",
    "        self.test_case = test_case\n",
    "        self.success = False\n",
    "        self.message = None\n",
    "        self.results = None\n",
    "    def forward(self, code):\n",
    "        try:\n",
    "            compiled = compile(code, 'test', 'exec')\n",
    "            exec(compiled, None, globals())\n",
    "        except Exception as e:\n",
    "            self.success = False\n",
    "            self.message = e\n",
    "            return False\n",
    "\n",
    "        test_suite = unittest.defaultTestLoader.loadTestsFromTestCase(self.test_case)\n",
    "        self.results = unittest.TextTestRunner().run(test_suite)\n",
    "        self.success = self.results.wasSuccessful()\n",
    "        return self.success\n",
    "\n",
    "\n",
    "class CodeGenPipeline(Pipeline):\n",
    "    def __init__(self, prompt_template, test_case):\n",
    "        self.p1 = PromptWithMKwargsNode(prompt_template=prompt_template)\n",
    "        self.eval = EvaluateNode(test_case=test_case)\n",
    "        self.code = None\n",
    "        self.model_output = None\n",
    "        self.success = False\n",
    "\n",
    "    def run(self, model_kwargs=None, prompt_kwargs=None):\n",
    "        # Intialize your Comet Experiment\n",
    "        experiment = comet_ml.Experiment(workspace=\"ckaiser\", project_name=\"llmops-test\")\n",
    "        experiment.add_tag(\"code-gen\")\n",
    "\n",
    "        # Run pipeline\n",
    "        self.model_output = self.p1.forward(model_kwargs=model_kwargs, prompt_kwargs=prompt_kwargs)\n",
    "        self.code = self.model_output.choices[0].text\n",
    "        self.success = self.eval.forward(self.code)\n",
    "\n",
    "        # Log metrics, parameters, and extra data to Comet\n",
    "        metrics = {\n",
    "            \"success\": self.success,\n",
    "            \"token_usage\": self.model_output.usage.\n",
    "            total_tokens\n",
    "        }\n",
    "\n",
    "        params = {\n",
    "            \"with_tests\": self.p1.prompt_template ==\n",
    "            code_gen_template_w_tests,\n",
    "            **model_kwargs\n",
    "        }\n",
    "\n",
    "        metadata = {\n",
    "            \"name\": self.p1.prompt_kwargs['name'],\n",
    "            \"description\": self.p1.prompt_kwargs['description'],\n",
    "            \"tests\": self.p1.prompt_kwargs['tests'],\n",
    "            \"prompt\": self.p1.prompt,\n",
    "            \"prompt_template\": self.p1.prompt_template,\n",
    "            \"usage.prompt_tokens\": self.model_output.usage.prompt_tokens,\n",
    "            \"usage.completion_tokens\": self.model_output.usage.completion_tokens,\n",
    "            \"usage.total_tokens\": self.model_output.usage.total_tokens,\n",
    "        }\n",
    "        \n",
    "        experiment.log_metrics(metrics)\n",
    "        experiment.log_parameters(params)\n",
    "        experiment.log_others(metadata)\n",
    "        return self.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in TESTS:\n",
    "    for template in [code_gen_template, code_gen_template_w_tests]:\n",
    "        for temperature in [0.0, 0.5, 1.0, 1.5]:\n",
    "            model_kwargs = { \"temperature\": temp }\n",
    "            pipeline = CodeGenPipeline(prompt_template=template, test_case=test['tests_class'])\n",
    "            success = pipeline.run(model_kwargs=model_kwargs, prompt_kwargs=test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('3.7.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6df8d4d5f9961af283fddcef9bd93cd9dd6348c412ee00d3ac64faf3dc94a62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
